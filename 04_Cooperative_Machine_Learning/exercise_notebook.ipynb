{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "exercise_notebook04.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "89622681ccf44a29a5ee86238ec1b074": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatSliderModel",
          "state": {
            "_view_name": "FloatSliderView",
            "style": "IPY_MODEL_72733168b7f44818bd0181f3932184c6",
            "_dom_classes": [],
            "description": "high conf",
            "step": 0.01,
            "_model_name": "FloatSliderModel",
            "orientation": "horizontal",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 0.6,
            "_view_count": null,
            "disabled": false,
            "_view_module_version": "1.5.0",
            "min": 0,
            "continuous_update": true,
            "readout_format": ".2f",
            "description_tooltip": null,
            "readout": true,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_af02600556c542ce914b0a0e7ee24236"
          }
        },
        "72733168b7f44818bd0181f3932184c6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "SliderStyleModel",
          "state": {
            "_view_name": "StyleView",
            "handle_color": null,
            "_model_name": "SliderStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "af02600556c542ce914b0a0e7ee24236": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "fb546e2fb5914cf8869b94849ec3055b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatSliderModel",
          "state": {
            "_view_name": "FloatSliderView",
            "style": "IPY_MODEL_62d374d9c8c645d6bba814a9d4cd6340",
            "_dom_classes": [],
            "description": "mid conf",
            "step": 0.01,
            "_model_name": "FloatSliderModel",
            "orientation": "horizontal",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 0.3,
            "_view_count": null,
            "disabled": false,
            "_view_module_version": "1.5.0",
            "min": 0,
            "continuous_update": true,
            "readout_format": ".2f",
            "description_tooltip": null,
            "readout": true,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_886cd54008c74f5d85960d427385eb9c"
          }
        },
        "62d374d9c8c645d6bba814a9d4cd6340": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "SliderStyleModel",
          "state": {
            "_view_name": "StyleView",
            "handle_color": null,
            "_model_name": "SliderStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "886cd54008c74f5d85960d427385eb9c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "77a29ca015a845b7a5980fe7f9ca93fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "RadioButtonsModel",
          "state": {
            "_options_labels": [
              "Wartortle",
              "Ivysaur",
              "Venusaur",
              "Blastoise",
              "Charmeleon",
              "Charizard",
              "Garbage"
            ],
            "_view_name": "RadioButtonsView",
            "style": "IPY_MODEL_8c69edf119da4d96b04ac149eb089d4e",
            "_dom_classes": [],
            "description": "Classes:",
            "_model_name": "RadioButtonsModel",
            "index": 0,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "disabled": false,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_8eb168f9e9194e71850c1ab9d9d85dd0"
          }
        },
        "8c69edf119da4d96b04ac149eb089d4e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "8eb168f9e9194e71850c1ab9d9d85dd0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": "max-content",
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "7d7c6b32806f436b99512def15e3e7a8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "state": {
            "_view_name": "ButtonView",
            "style": "IPY_MODEL_db1897700678406090b919b97d8d5ca0",
            "_dom_classes": [],
            "description": "Retrain",
            "_model_name": "ButtonModel",
            "button_style": "",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "tooltip": "",
            "_view_count": null,
            "disabled": false,
            "_view_module_version": "1.5.0",
            "layout": "IPY_MODEL_eae6206226014378b948ffbdf67722f2",
            "_model_module": "@jupyter-widgets/controls",
            "icon": ""
          }
        },
        "db1897700678406090b919b97d8d5ca0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ButtonStyleModel",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "button_color": null,
            "font_weight": "",
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "eae6206226014378b948ffbdf67722f2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c9f5d5c228cc4563bd9b3740b3b0654b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "state": {
            "_view_name": "ButtonView",
            "style": "IPY_MODEL_cc738ac917a7459fbd74ef051e925644",
            "_dom_classes": [],
            "description": "Predict",
            "_model_name": "ButtonModel",
            "button_style": "",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "tooltip": "",
            "_view_count": null,
            "disabled": false,
            "_view_module_version": "1.5.0",
            "layout": "IPY_MODEL_e394deb452ac4bbcaaf8ae69f9e86347",
            "_model_module": "@jupyter-widgets/controls",
            "icon": ""
          }
        },
        "cc738ac917a7459fbd74ef051e925644": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ButtonStyleModel",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "button_color": null,
            "font_weight": "",
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "e394deb452ac4bbcaaf8ae69f9e86347": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e999b8c3bdf64f1791886ed434c69bb6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "state": {
            "_view_name": "ButtonView",
            "style": "IPY_MODEL_2c0381f61d9f46aea52450069832f32f",
            "_dom_classes": [],
            "description": "Next",
            "_model_name": "ButtonModel",
            "button_style": "",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "tooltip": "",
            "_view_count": null,
            "disabled": false,
            "_view_module_version": "1.5.0",
            "layout": "IPY_MODEL_521d1aa5bf0842b582ccdb5241bb9a01",
            "_model_module": "@jupyter-widgets/controls",
            "icon": ""
          }
        },
        "2c0381f61d9f46aea52450069832f32f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ButtonStyleModel",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "button_color": null,
            "font_weight": "",
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "521d1aa5bf0842b582ccdb5241bb9a01": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "60d22a5210e3497581f7bd68d83ab770": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ImageModel",
          "state": {
            "_view_name": "ImageView",
            "_dom_classes": [],
            "_model_name": "ImageModel",
            "format": "jpeg",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "width": "400",
            "_view_module_version": "1.5.0",
            "layout": "IPY_MODEL_ad29499762fc4509ac0e191b7d0ee391",
            "height": "400",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "ad29499762fc4509ac0e191b7d0ee391": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hVDWyllXxKm2",
        "outputId": "5daa7c76-18f4-40ac-d983-b171aaedf948"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t4iAHSIEyFxv"
      },
      "source": [
        "! rm -rf /content/data_labeled/\n",
        "! rm -rf /content/__MACOSX/\n",
        "! ls -la\n",
        "\n",
        "! unzip '/content/gdrive/MyDrive/data_labeled.zip'\n",
        "! unzip '/content/gdrive/MyDrive/imgs.zip'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kXVkfUpgwM_w"
      },
      "source": [
        "# Interactive Machine Learning - Exercise 03\n",
        "\n",
        "In this exercise we will learn about cooperative machine learning.\n",
        "Our goal is it to build a very basic cooperative machine learning user interface and use it to extend our Pokedex model from the last exercise.\n",
        "\n",
        "The steps you are going to cover are as follows:\n",
        "* Pretrain our Pokedex model with the original data\n",
        "* Manually label a small bit of new data\n",
        "* Train our model on the new data\n",
        "* Use the model in a cooperative workflow to annotate the rest of the dataset\n",
        "\n",
        "Please read each exercise carefully before you start coding! You will find a number in the comments before each step of coding you will do. Please refer to these numbers if you have any questions.\n",
        "\n",
        "## 0. Import the libraries\n",
        "As always we are providing a list useful packages in the import section below.\n",
        "Keep in mind that you can import additional libraries at any time and that you do not need to use all the imports if you know another solution for a given task."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f-jWl0qowM_w"
      },
      "source": [
        "import ipywidgets as widgets\n",
        "import os\n",
        "import numpy as np\n",
        "import glob\n",
        "import random\n",
        "from IPython.display import Image\n",
        "from ipywidgets import interact_manual\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.applications.vgg16 import VGG16, preprocess_input\n",
        "from tensorflow.keras.layers import Flatten, Dense, GlobalAveragePooling2D, Input\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from shutil import copyfile\n",
        "\n",
        "import os\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oK2QLPWZwM_w"
      },
      "source": [
        "## 1. Pretrain the model\n",
        "In this part we are going to pretrain our model on the pokemon images you already know.\n",
        "To this end we will use the same VGG16 model as last week with the following training procedure:\n",
        "\n",
        "Preprocessing:\n",
        "* Imagesize (224,224)\n",
        "* Vgg16 standard preprocessing from the Keras framework\n",
        "\n",
        "Datasplit:\n",
        "* Use 90% of the data to train and 10% to valitdate your results\n",
        "\n",
        "Training 1:\n",
        "* Initialize the model with the imagenet weights\n",
        "* Freeze all convolution layers\n",
        "* Train the model using the following settings:\n",
        " * 5 Epochs\n",
        " * Adam Optimizer with default Parameters\n",
        " * categorical cross entropy loss\n",
        " * Batchsize 32\n",
        "\n",
        "Training 2:\n",
        "*  Unfreeze the last two convolutional Blocks\n",
        "*  Continue training with the following settings:\n",
        " * 10 Epochs\n",
        " * Adam Optimizer with a learning rate of 0.0001\n",
        " * Batchsize of 32\n",
        "\n",
        "A convolutional Block in the VGG16 architecture consists of 2 to 3 Conv Layers and on Pooling layer.\n",
        "You can access a models layer directly via `model.layers`.\n",
        "Read up on how to freeze layers [here](https://keras.io/guides/transfer_learning/), in case you did not use this technique in the last exercise.\n",
        "Your model should achieve a validation accuracy of close to 100% ."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MpJ92jkcwM_x",
        "outputId": "58b53115-a699-423e-9440-e27df6ae453e"
      },
      "source": [
        "# 1. Load data for pretraining and apply preprocessing\n",
        "img_paths = [f'/content/imgs/{x}' for x in os.listdir('/content/imgs')]\n",
        "print(len(img_paths), img_paths[0])\n",
        "\n",
        "labels = []\n",
        "for path in img_paths:\n",
        "    label = path.split('/')[-1].split('_')[0]\n",
        "    labels.append(label)\n",
        "\n",
        "classes = list(set(labels))\n",
        "print('distinct classes: ', classes)\n",
        "n_classes = len(classes)\n",
        "\n",
        "mapping = {k : classes.index(k) for k in classes}\n",
        "print('mapping: ', mapping)\n",
        "labels = [mapping[x] for x in labels]\n",
        "print('encoded labels: ', labels[:3])\n",
        "\n",
        "def one_hot(labels, depth):\n",
        "    one_hot_vectors = []\n",
        "    for label in labels:\n",
        "        one_hot_vector = [0] * depth\n",
        "        one_hot_vector[label] = 1\n",
        "        one_hot_vectors.append(one_hot_vector)\n",
        "    return one_hot_vectors\n",
        "\n",
        "one_hot_labels = one_hot(labels, depth=8)\n",
        "print('one hot encoded labels: ', one_hot_labels[:3])\n",
        "one_hot_labels = np.array(one_hot_labels)\n",
        "\n",
        "def read_image(fname):\n",
        "    image = tf.io.read_file(fname)\n",
        "    image = tf.image.decode_jpeg(image, channels=3)\n",
        "    image = tf.cast(image, tf.float32)\n",
        "    image = (image/127.5) - 1\n",
        "    image = tf.image.resize(images=image, size=(224, 224))\n",
        "    proto_tensor = tf.make_tensor_proto(image)\n",
        "    image = tf.make_ndarray(proto_tensor)\n",
        "\n",
        "    return image\n",
        "\n",
        "x = []\n",
        "for im_path in img_paths:\n",
        "    img = read_image(im_path)\n",
        "    x.append(img)\n",
        "\n",
        "x = np.array(x)\n",
        "print('x len:', len(x))\n",
        "print('x instance shape: ', x[0].shape)\n",
        "\n",
        "# 2. Split data into training and test partition\n",
        "x_train, x_val, y_train, y_val = train_test_split(x, one_hot_labels,\n",
        "                                                  test_size=0.10,\n",
        "                                                  random_state=42)\n",
        "print(len(x_train), len(x_val), len(y_train), len(y_val))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "347 /content/imgs/VAPOREON_8.jpg\n",
            "distinct classes:  ['CHARMANDER', 'EEVEE', 'FLAREON', 'PIKACHU', 'BULBASAUR', 'VAPOREON', 'SQUIRTLE', 'JOLTEON']\n",
            "mapping:  {'CHARMANDER': 0, 'EEVEE': 1, 'FLAREON': 2, 'PIKACHU': 3, 'BULBASAUR': 4, 'VAPOREON': 5, 'SQUIRTLE': 6, 'JOLTEON': 7}\n",
            "encoded labels:  [5, 7, 4]\n",
            "one hot encoded labels:  [[0, 0, 0, 0, 0, 1, 0, 0], [0, 0, 0, 0, 0, 0, 0, 1], [0, 0, 0, 0, 1, 0, 0, 0]]\n",
            "x len: 347\n",
            "x instance shape:  (224, 224, 3)\n",
            "312 35 312 35\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xks3k4rj4yiv",
        "outputId": "25690782-6d02-41ba-8d87-b28dcc8d9baa"
      },
      "source": [
        "# 3. Define network\n",
        "base_model = tf. keras.applications.VGG16(weights=\"imagenet\",\n",
        "                                          include_top = False,\n",
        "                                          input_shape=(224, 224, 3))\n",
        "input_ = Input(shape=(224, 224, 3))\n",
        "x_ = base_model(input_, training=False)\n",
        "x_ = Flatten()(x_)\n",
        "x_ = Dense(4096, activation='relu')(x_)\n",
        "x_ = Dense(4096, activation='relu')(x_)\n",
        "output_ = Dense(n_classes, activation='softmax')(x_)\n",
        "pokexedx = Model(input_, output_)\n",
        "pokexedx.summary()\n",
        "\n",
        "# 4. Freeze weights and perform training step 1\n",
        "pokexedx.trainable = False\n",
        "\n",
        "opt = Adam()\n",
        "pokexedx.compile(optimizer=opt,\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics='acc')\n",
        "pokexedx.fit(x=x_train,\n",
        "          y=y_train, \n",
        "          epochs=5,\n",
        "          batch_size=32,\n",
        "          validation_data=(x_val, y_val))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "58892288/58889256 [==============================] - 1s 0us/step\n",
            "Model: \"functional_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_2 (InputLayer)         [(None, 224, 224, 3)]     0         \n",
            "_________________________________________________________________\n",
            "vgg16 (Functional)           (None, 7, 7, 512)         14714688  \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 25088)             0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 4096)              102764544 \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 4096)              16781312  \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 8)                 32776     \n",
            "=================================================================\n",
            "Total params: 134,293,320\n",
            "Trainable params: 134,293,320\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/5\n",
            "10/10 [==============================] - 3s 282ms/step - loss: 2.1700 - acc: 0.1827 - val_loss: 2.3125 - val_acc: 0.0571\n",
            "Epoch 2/5\n",
            "10/10 [==============================] - 1s 121ms/step - loss: 2.1700 - acc: 0.1827 - val_loss: 2.3125 - val_acc: 0.0571\n",
            "Epoch 3/5\n",
            "10/10 [==============================] - 1s 122ms/step - loss: 2.1700 - acc: 0.1827 - val_loss: 2.3125 - val_acc: 0.0571\n",
            "Epoch 4/5\n",
            "10/10 [==============================] - 1s 121ms/step - loss: 2.1700 - acc: 0.1827 - val_loss: 2.3125 - val_acc: 0.0571\n",
            "Epoch 5/5\n",
            "10/10 [==============================] - 1s 122ms/step - loss: 2.1700 - acc: 0.1827 - val_loss: 2.3125 - val_acc: 0.0571\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fc6541b5828>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tJ9BKa3q4SK1",
        "outputId": "141a2541-cc39-4dd6-9f51-4b5eea3b49db"
      },
      "source": [
        "# 5. Unfreeze weights and perform training step 2\n",
        "base_model.summary()\n",
        "\n",
        "print('\\n'*3)\n",
        "for layer in base_model.layers:\n",
        "    if layer.name.startswith('block5_conv') or \\\n",
        "    layer.name.startswith('block4_conv'):\n",
        "        print('unfreezing layer: ', layer.name)\n",
        "        layer.trainable = True\n",
        "print('\\n'*3)\n",
        "\n",
        "opt = Adam(learning_rate=0.0001)\n",
        "pokexedx.compile(optimizer=opt,\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics='acc')\n",
        "pokexedx.fit(x=x_train,\n",
        "                    y=y_train, \n",
        "                    epochs=10,\n",
        "                    batch_size=32,\n",
        "                    validation_data=(x_val, y_val))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"vgg16\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_16 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
            "_________________________________________________________________\n",
            "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
            "_________________________________________________________________\n",
            "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
            "_________________________________________________________________\n",
            "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
            "_________________________________________________________________\n",
            "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
            "_________________________________________________________________\n",
            "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
            "_________________________________________________________________\n",
            "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
            "_________________________________________________________________\n",
            "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
            "_________________________________________________________________\n",
            "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
            "_________________________________________________________________\n",
            "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
            "_________________________________________________________________\n",
            "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
            "_________________________________________________________________\n",
            "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
            "=================================================================\n",
            "Total params: 14,714,688\n",
            "Trainable params: 0\n",
            "Non-trainable params: 14,714,688\n",
            "_________________________________________________________________\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "unfreezing layer:  block4_conv1\n",
            "unfreezing layer:  block4_conv2\n",
            "unfreezing layer:  block4_conv3\n",
            "unfreezing layer:  block5_conv1\n",
            "unfreezing layer:  block5_conv2\n",
            "unfreezing layer:  block5_conv3\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Epoch 1/10\n",
            "10/10 [==============================] - 1s 148ms/step - loss: 2.1386 - acc: 0.1058 - val_loss: 2.1512 - val_acc: 0.1429\n",
            "Epoch 2/10\n",
            "10/10 [==============================] - 1s 134ms/step - loss: 2.1386 - acc: 0.1058 - val_loss: 2.1512 - val_acc: 0.1429\n",
            "Epoch 3/10\n",
            "10/10 [==============================] - 1s 133ms/step - loss: 2.1386 - acc: 0.1058 - val_loss: 2.1512 - val_acc: 0.1429\n",
            "Epoch 4/10\n",
            "10/10 [==============================] - 1s 134ms/step - loss: 2.1386 - acc: 0.1058 - val_loss: 2.1512 - val_acc: 0.1429\n",
            "Epoch 5/10\n",
            "10/10 [==============================] - 1s 134ms/step - loss: 2.1386 - acc: 0.1058 - val_loss: 2.1512 - val_acc: 0.1429\n",
            "Epoch 6/10\n",
            "10/10 [==============================] - 1s 134ms/step - loss: 2.1386 - acc: 0.1058 - val_loss: 2.1512 - val_acc: 0.1429\n",
            "Epoch 7/10\n",
            "10/10 [==============================] - 1s 135ms/step - loss: 2.1386 - acc: 0.1058 - val_loss: 2.1512 - val_acc: 0.1429\n",
            "Epoch 8/10\n",
            "10/10 [==============================] - 1s 135ms/step - loss: 2.1386 - acc: 0.1058 - val_loss: 2.1512 - val_acc: 0.1429\n",
            "Epoch 9/10\n",
            "10/10 [==============================] - 1s 138ms/step - loss: 2.1386 - acc: 0.1058 - val_loss: 2.1512 - val_acc: 0.1429\n",
            "Epoch 10/10\n",
            "10/10 [==============================] - 1s 137ms/step - loss: 2.1386 - acc: 0.1058 - val_loss: 2.1512 - val_acc: 0.1429\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f87ba0ec940>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vUIZ7cOHwM_x"
      },
      "source": [
        "## 2. Pretrain the model\n",
        "Now that we have our initial model we are going to extend it with some more pokemon.\n",
        "[Here](https://megastore.uni-augsburg.de/get/OxpI3M_JyU/) you will find roughly 6000 images of the following Pokemon:\n",
        "* Blastoise\n",
        "* Charizard\n",
        "* Charmeleon\n",
        "* Ivysaur\n",
        "* Venusaur\n",
        "* Wartortle\n",
        "\n",
        "Unfortunately images are not labeled yet. To speed things up a bit we are only going to label a small part of the data ourselves, and then build a model to help us doing the rest.\n",
        "(Actually this will probably not be faster, but more fun anyway :) ).\n",
        "In your project directory you will find a 'data_labled' folder, which we will use to store the labeled data.\n",
        "This time we will use the folder structure to create our labels and train / validation partitions.\n",
        "Inside the folder you will therefore find a 'train' and an 'val' folder, each of them containing subfolders for each class.\n",
        "\n",
        "In the following step you should at first manually pick at least 5 examples per class and copy them from the 'data' folder to the train partition of the 'data_labeled' folder.\n",
        "To then take full advantage of the current way the data is structured, we will use keras data generators in combination with the `flow_from_directory` to dynamically read the input data and feed it to our model.\n",
        "You can find an example of such data generators [here](https://keras.io/api/preprocessing/image/#flowfromdirectory-method).\n",
        "\n",
        "Specifically we are going to write a function `train_loop()` which creates two data generators (one for training and one for validation) and trains a model for the new Images on features extracted from our current Pokexedx model.\n",
        "To this end you can simply rebuild the structure of the original model, but replace the number of output classes.\n",
        "To load the weights you can then use the following code snippet:\n",
        "`model.layers[-1]._name = 'new_output'`</br>\n",
        "`model.load_weights(weight_path, by_name=True)`</br>\n",
        "\n",
        "Freeze all layers but the dense layers, we will only need those and want to speed up the training process a bit."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rUTw8xN7AZjs"
      },
      "source": [
        "weight_path = '/content/model_weights'\n",
        "pokexedx.save_weights(weight_path,\n",
        "                             overwrite=True,\n",
        "                             save_format='h5',\n",
        ")"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-tc0jmQnBwag"
      },
      "source": [
        "! wget https://megastore.uni-augsburg.de/get/OxpI3M_JyU/data.zip\n",
        "! ls -la"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6oEzuRRGCIBI"
      },
      "source": [
        "! unzip '/content/data.zip'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_jKXXZsLwM_x",
        "outputId": "8d888458-068e-48a1-8f8d-1eb6f4d7a449"
      },
      "source": [
        "# 6. Copy at least 5 images per class from the data folder to the correct partition in the data_labeled folder\n",
        "# made by hand?\n",
        "\n",
        "# 7. Write a function train_loop()\n",
        "classes = [x for x in os.listdir('/content/data_labeled/train') if x != '.DS_Store']\n",
        "n_classes = len(classes)\n",
        "print(n_classes, classes) \n",
        "\n",
        "def train_loop(n_classes, base_model, weights_path, verbose=0):\n",
        "    # 8. Build model\n",
        "    input_ = Input(shape=(224, 224, 3))\n",
        "    x_ = base_model(input_, training=False)\n",
        "    x_ = Flatten()(x_)\n",
        "    x_ = Dense(4096, activation='relu')(x_)\n",
        "    x_ = Dense(4096, activation='relu')(x_)\n",
        "    output_ = Dense(n_classes, activation='softmax')(x_)\n",
        "    model = Model(input_, output_)\n",
        "    if verbose == 1:\n",
        "        model.summary()\n",
        "\n",
        "    # 9. Load weights\n",
        "    model.load_weights(weights_path,\n",
        "                       skip_mismatch=True,\n",
        "                       by_name=True)\n",
        "\n",
        "    opt = Adam(learning_rate=0.0001)\n",
        "    model.compile(optimizer=opt,\n",
        "                    loss='categorical_crossentropy',\n",
        "                    metrics='acc')\n",
        "\n",
        "    # 10. Build data generators\n",
        "    train_datagen = ImageDataGenerator(rescale=1./255, vertical_flip=True)\n",
        "    test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "    train_generator = train_datagen.flow_from_directory(\n",
        "            '/content/data_labeled/train',\n",
        "            target_size=(224, 224),\n",
        "            batch_size=32,\n",
        "            class_mode='categorical')\n",
        "    validation_generator = test_datagen.flow_from_directory(\n",
        "            '/content/data_labeled/val',\n",
        "            target_size=(224, 224),\n",
        "            batch_size=32,\n",
        "            class_mode='categorical')\n",
        "\n",
        "    # 11. Fit the model to the data for a few epochs\n",
        "    model.fit(train_generator,\n",
        "              epochs=10,\n",
        "              verbose=verbose,\n",
        "              validation_data=validation_generator)\n",
        "    weights_path = '/content/model_new_weights'\n",
        "    model.save_weights(weights_path,\n",
        "                        overwrite=True,\n",
        "                        save_format='h5',\n",
        "    )\n",
        "    model.save('/content/saved_model')\n",
        "\n",
        "# 12. Call train loop\n",
        "vgg_model = tf.keras.applications.VGG16(weights='imagenet',\n",
        "                                        include_top=False,\n",
        "                                        input_shape=(224, 224, 3))\n",
        "vgg_model.trainable = False\n",
        "\n",
        "train_loop(n_classes=n_classes,\n",
        "            base_model=base_model,\n",
        "            verbose=1,\n",
        "            weights_path='/content/model_weights')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "6 ['Wartortle', 'Ivysaur', 'Venusaur', 'Blastoise', 'Charmeleon', 'Charizard']\n",
            "Model: \"functional_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_4 (InputLayer)         [(None, 224, 224, 3)]     0         \n",
            "_________________________________________________________________\n",
            "vgg16 (Functional)           (None, 7, 7, 512)         14714688  \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 25088)             0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 4096)              102764544 \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 4096)              16781312  \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 6)                 24582     \n",
            "=================================================================\n",
            "Total params: 134,285,126\n",
            "Trainable params: 119,570,438\n",
            "Non-trainable params: 14,714,688\n",
            "_________________________________________________________________\n",
            "Found 30 images belonging to 6 classes.\n",
            "Found 90 images belonging to 6 classes.\n",
            "Epoch 1/10\n",
            "1/1 [==============================] - 2s 2s/step - loss: 1.8731 - acc: 0.2667 - val_loss: 3.8601 - val_acc: 0.1778\n",
            "Epoch 2/10\n",
            "1/1 [==============================] - 0s 437ms/step - loss: 2.9063 - acc: 0.1667 - val_loss: 3.1555 - val_acc: 0.1778\n",
            "Epoch 3/10\n",
            "1/1 [==============================] - 0s 443ms/step - loss: 2.0882 - acc: 0.3667 - val_loss: 3.0153 - val_acc: 0.3000\n",
            "Epoch 4/10\n",
            "1/1 [==============================] - 0s 438ms/step - loss: 1.7490 - acc: 0.4667 - val_loss: 2.9155 - val_acc: 0.4111\n",
            "Epoch 5/10\n",
            "1/1 [==============================] - 0s 437ms/step - loss: 1.6990 - acc: 0.6667 - val_loss: 2.9473 - val_acc: 0.2889\n",
            "Epoch 6/10\n",
            "1/1 [==============================] - 0s 441ms/step - loss: 1.5641 - acc: 0.6333 - val_loss: 1.9708 - val_acc: 0.4444\n",
            "Epoch 7/10\n",
            "1/1 [==============================] - 0s 447ms/step - loss: 0.6102 - acc: 0.7667 - val_loss: 1.2698 - val_acc: 0.5889\n",
            "Epoch 8/10\n",
            "1/1 [==============================] - 0s 440ms/step - loss: 0.1371 - acc: 1.0000 - val_loss: 1.3841 - val_acc: 0.4444\n",
            "Epoch 9/10\n",
            "1/1 [==============================] - 0s 441ms/step - loss: 0.0984 - acc: 1.0000 - val_loss: 1.8369 - val_acc: 0.3333\n",
            "Epoch 10/10\n",
            "1/1 [==============================] - 0s 444ms/step - loss: 0.1201 - acc: 0.9667 - val_loss: 2.0337 - val_acc: 0.3111\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
            "INFO:tensorflow:Assets written to: /content/saved_model/assets\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jN38aot3wM_x"
      },
      "source": [
        "## 3. Interactive UI\n",
        "\n",
        "In this part of the exercise we are going to put our pretrained model to good use by employing it in a cooperative workflow.\n",
        "To this end we gonna build a minimal cooperative machine learning using interface in this python notebook.\n",
        "Our user interface will consist of the following components:\n",
        "\n",
        "* (optional) A progressbar to keep to motivation up\n",
        "* A slider to set a high confidence threshold\n",
        "* A slider to set the mid confidence threshold\n",
        "* Some radio buttons to choose the label\n",
        "* A button to save the annotation and label and show the next image\n",
        "* A button to retrain our model\n",
        "* A button to use our model to predict our dataset\n",
        "\n",
        "The final our UI should look a like this:\n",
        "\n",
        "![img](https://hcm-lab.de/cloud/index.php/s/ak3txGXepnt9NxS/preview)\n",
        "\n",
        "The 'retrain' button should call the `train_loop()`  function from before to retrain the model on all labeled data.\n",
        "The 'predict' button should create a list of predictions for all unlabeled images.\n",
        "All predictions that are above the high confidence threshold, set by the respective slider, should be automatically accepted as correct label and copied to the respective folders in the training data folder.\n",
        "Additionally you should implement a garbage label to delete unfitting images.\n",
        "Potential reasons to consider an Image as garbage are if no Pokemon is visible, too many Pokemon are visible, non of the Pokemon we want to train are visible, the Imagefile is broken etc.\n",
        "When you are pressing the 'next' button the current image should be copied to the right folder in the training dataset, depending on the current value of the radio button.\n",
        "Afterwards the next image should be chosen from all predicted images, where the confidence is greater or equal than the value set by the mid_threshold slider.\n",
        "The current value of the radiobutton should then be set to the prediction for this respective image.\n",
        "Optionally you can also implement a progressbar to track your progress for you annotations.\n",
        "\n",
        "You can use the ipywidgets library to create the UI.\n",
        "You can find an IPython tutorial [here](https://towardsdatascience.com/interactive-controls-for-jupyter-notebooks-f5c94829aee6) and the api documentation [here](https://towardsdatascience.com/interactive-controls-for-jupyter-notebooks-f5c94829aee6).\n",
        "Note, that Pycharm might not play well with the the widgets in all scenarios. It's best to view them in the browser by visting: http://localhost:8888 after you started your notebook."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350,
          "referenced_widgets": [
            "89622681ccf44a29a5ee86238ec1b074",
            "72733168b7f44818bd0181f3932184c6",
            "af02600556c542ce914b0a0e7ee24236",
            "fb546e2fb5914cf8869b94849ec3055b",
            "62d374d9c8c645d6bba814a9d4cd6340",
            "886cd54008c74f5d85960d427385eb9c",
            "77a29ca015a845b7a5980fe7f9ca93fc",
            "8c69edf119da4d96b04ac149eb089d4e",
            "8eb168f9e9194e71850c1ab9d9d85dd0",
            "7d7c6b32806f436b99512def15e3e7a8",
            "db1897700678406090b919b97d8d5ca0",
            "eae6206226014378b948ffbdf67722f2",
            "c9f5d5c228cc4563bd9b3740b3b0654b",
            "cc738ac917a7459fbd74ef051e925644",
            "e394deb452ac4bbcaaf8ae69f9e86347",
            "e999b8c3bdf64f1791886ed434c69bb6",
            "2c0381f61d9f46aea52450069832f32f",
            "521d1aa5bf0842b582ccdb5241bb9a01",
            "60d22a5210e3497581f7bd68d83ab770",
            "ad29499762fc4509ac0e191b7d0ee391"
          ]
        },
        "id": "dmUgbb4xwM_x",
        "outputId": "e3e808fa-a7cc-4023-885b-bc7fc894000a"
      },
      "source": [
        "# 13. Build UI\n",
        "import ipywidgets as widgets\n",
        "from ipywidgets import interact, interact_manual\n",
        "from IPython.display import Image\n",
        "from IPython.display import clear_output\n",
        "from functools import partial\n",
        "import shutil\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "import threading\n",
        "np.set_printoptions(suppress=True)\n",
        "\n",
        "\n",
        "class Button_Callback:\n",
        "    def __init__(self):\n",
        "        self.preds = None\n",
        "        self.X = None\n",
        "        self.cls = None\n",
        "\n",
        "    def button_retr_callback(self, b):\n",
        "        train_loop(n_classes=n_classes,\n",
        "                    base_model=base_model,\n",
        "                    weights_path='/content/model_new_weights')\n",
        "\n",
        "    def radio_callback(self, b):\n",
        "        self.cls = radio_buttons.value\n",
        "        # print('='*50)\n",
        "        # print('Assigned new class:', self.cls)\n",
        "        if self.cls == 'Garbage':\n",
        "            # print('Garbage class assigned, removing from unlabeled data...')\n",
        "            os.remove(self.im_src)\n",
        "            pass\n",
        "        else:\n",
        "            im_dst = f'/content/data_labeled/train/{self.cls}/{self.im_id}'\n",
        "            # print('saving...', self.im_src, im_dst)\n",
        "            shutil.copy(self.im_src, im_dst)\n",
        "        print('next...')\n",
        "\n",
        "\n",
        "    def button_next_callback(self, b):\n",
        "        if self.mid_confs:\n",
        "            self.im_src, pred_class = self.mid_confs[0]\n",
        "            self.mid_confs.pop(0)\n",
        "\n",
        "            self.im_id = self.im_src.split('/')[-1]\n",
        "            # print('Predicted class: ', pred_class)\n",
        "            file = open(self.im_src, \"rb\")\n",
        "            image = file.read()\n",
        "            im_widget = widgets.Image(value=image,\n",
        "                                    format='jpeg',\n",
        "                                    width=400,\n",
        "                                    height=400)\n",
        "            display(im_widget)\n",
        "            file.close()\n",
        "            print('You have 10 seconds to change the label!')\n",
        "            time.sleep(10)\n",
        "            im_widget.close()\n",
        "        else:\n",
        "            print('annotation done.')\n",
        "        \n",
        "    def button_pred_callback(self, b):\n",
        "        # print('predicting...')\n",
        "        path = '/content/saved_model'\n",
        "        retr_model = tf.keras.models.load_model(path)\n",
        "        # retr_model.summary()\n",
        "\n",
        "        all_labeled = [x.split('/')[-1] for x in glob.glob('/content/data_labeled/train/*/*')]\n",
        "        self.X = ['/content/data/' + x for x in os.listdir('/content/data/') if x not in all_labeled]\n",
        "\n",
        "        imgs_to_pred = []\n",
        "        for x in self.X[:3]:\n",
        "            try:\n",
        "                imgs_to_pred.append(read_image(x))\n",
        "            except Exception as e:\n",
        "                ...\n",
        "                # print(f'Exception: {e}, image path: {x}')\n",
        "        # print('creating array of images...')\n",
        "        imgs_to_pred = np.array(imgs_to_pred)\n",
        "        self.preds = retr_model.predict(imgs_to_pred)\n",
        "        # print('predicted...', self.preds)\n",
        "        self.pred_labels = [np.argmax(x) for x in self.preds]\n",
        "\n",
        "        self.mid_confs = []\n",
        "\n",
        "        for idx, pred in enumerate(self.preds):\n",
        "            pred = pred[np.argmax(pred)]\n",
        "            if pred >= high_conf_slider.value:\n",
        "                # print(pred)\n",
        "                im_src = self.X[idx]\n",
        "                im_id = im_src.split('/')[-1]\n",
        "                # print(im_src, im_id)\n",
        "                # print(self.pred_labels[idx], classes)\n",
        "\n",
        "                pred_class = classes[self.pred_labels[idx]]\n",
        "                im_dst = f'/content/data_labeled/train/{pred_class}/{im_id}'\n",
        "                # print(im_src, im_dst)\n",
        "                shutil.copy(im_src, im_dst)\n",
        "            elif high_conf_slider.value > pred >= mid_conf_slider.value:\n",
        "                # print('mid: ', pred)\n",
        "                im_src = self.X[idx]\n",
        "                im_id = im_src.split('/')[-1]\n",
        "                # print(im_src, im_id)\n",
        "                # print(self.pred_labels[idx], classes)\n",
        "                pred_class = classes[self.pred_labels[idx]]\n",
        "                self.mid_confs.append([im_src, pred_class])\n",
        "                # print(len(self.mid_confs), self.mid_confs)\n",
        "        # print('done.')\n",
        "\n",
        "callbacks = Button_Callback()\n",
        "\n",
        "high_conf_slider = widgets.FloatSlider(min=0.0, max=1.0, step=0.01, value=0.6, description='high conf')\n",
        "mid_conf_slider = widgets.FloatSlider(min=0.0, max=1.0, step=0.01, value=0.3, description='mid conf')\n",
        "\n",
        "button_next = widgets.Button(description=\"Next\")\n",
        "button_next.on_click(callbacks.button_next_callback)\n",
        "\n",
        "button_retr = widgets.Button(description=\"Retrain\")\n",
        "button_retr.on_click(callbacks.button_retr_callback)\n",
        "\n",
        "button_pred = widgets.Button(description=\"Predict\")\n",
        "button_pred.on_click(callbacks.button_pred_callback)\n",
        "\n",
        "classes_gargabe = classes + ['Garbage']\n",
        "radio_buttons = widgets.RadioButtons(\n",
        "            options=classes_gargabe,\n",
        "            layout={'width': 'max-content'},\n",
        "            description='Classes:'\n",
        "        )\n",
        "\n",
        "radio_buttons.observe(callbacks.radio_callback, names=['value'])\n",
        "\n",
        "display(high_conf_slider)\n",
        "display(mid_conf_slider)\n",
        "display(radio_buttons)\n",
        "display(button_retr)\n",
        "display(button_pred)\n",
        "display(button_next)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "89622681ccf44a29a5ee86238ec1b074",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "FloatSlider(value=0.6, description='high conf', max=1.0, step=0.01)"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fb546e2fb5914cf8869b94849ec3055b",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "FloatSlider(value=0.3, description='mid conf', max=1.0, step=0.01)"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "77a29ca015a845b7a5980fe7f9ca93fc",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "RadioButtons(description='Classes:', layout=Layout(width='max-content'), options=('Wartortle', 'Ivysaur', 'Ven"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7d7c6b32806f436b99512def15e3e7a8",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Button(description='Retrain', style=ButtonStyle())"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c9f5d5c228cc4563bd9b3740b3b0654b",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Button(description='Predict', style=ButtonStyle())"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e999b8c3bdf64f1791886ed434c69bb6",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Button(description='Next', style=ButtonStyle())"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "60d22a5210e3497581f7bd68d83ab770",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Image(value=b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x02\\x00\\x00d\\x00d\\x00\\x00\\xff\\xec\\x00\\x11Ducky\\x00\\x01\\x00\\"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "You have 10 seconds to change the label!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "source": [
        "![GUI](screenshot.png)"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dXWl5tGCwM_x"
      },
      "source": [
        "## 4. repeat(annotate, train, predict)\n",
        "After you are done creating the UI, we are now going to label the whole dataset together with our model.\n",
        "To this end use your model to predict and improve iteratively in the following manner:\n",
        "\n",
        "Set the high confidence slider to a value greater or equal than 0.95 and the mid confidence slider to at least 0.8\n",
        "\n",
        "Repeat 3 times:\n",
        "\n",
        "* Call automatic prediction\n",
        "* Check images that have been above the maximum confidence threshold manually by looking at the content of the respective folders. Make corrections if necessary.\n",
        "* Annotate remaining images that have been over the mid confidence score\n",
        "* Retrain you model\n",
        "\n",
        "Do you notice any change in the amount of images you have to annotate each time?\n",
        "\n",
        "Repeat till all data is annotated:\n",
        "\n",
        "* Call automatic prediction\n",
        "* Annotate remaining images that have been over the mid confidence score\n",
        "* Retrain you model\n",
        "* Adjust both confidence scores based on how much you trust your model\n",
        "\n",
        "Describe your subjective impression of the annotation process. Did you have the feeling, that the cooperative workflow is helpful?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GKRFSDbqscB_"
      },
      "source": [
        "If base model accuracy is not sufficient to differentiate all possible classes and it overfits on only 1 class, then you have to set high confidence value to 1.0, because the model predicts only one class \"Ivysaur\" with the confidence of >= .99 for each image. Therefore you need to manually check all predictions.\n",
        "\n",
        "From the other hand, if the model is performing well it is a very helpful tool to manually annotate images that model cannot assign class with desired confidence.\n"
      ]
    }
  ]
}